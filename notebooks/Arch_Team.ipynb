{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Adds the parent directory to the path to access `mymodule`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core VAE Architecture Model:\n",
    "\n",
    "    - Encoder:\n",
    "        - Input vector of size 60664\n",
    "        - Output Mean and Variance vectors of size 64 for latent space\n",
    "\n",
    "    - Decoder:\n",
    "        - Input latent space vector of size 64\n",
    "        - Output Reconstrcuted input of size 60664\n",
    "    \n",
    "    - Using Xavier weight initialization\n",
    "\n",
    "    - Using ReLU activation functions throughout\n",
    "\n",
    "    - Addition of batch normalization layers after each activation\n",
    "\n",
    "    - Usage:\n",
    "        - Set model in trainer: model = Arch_Model.VAE()\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mmvae.models.utils as utils\n",
    "import mmvae.models as M\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        #Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(60664, 1024),\n",
    "            nn.BatchNorm1d(1024, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #Latent Space\n",
    "        self.fc_mu = nn.Linear(64, 64)\n",
    "        self.fc_var = nn.Linear(64, 64)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 60664),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        utils._submodules_init_weights_xavier_uniform_(self.encoder)\n",
    "        utils._submodules_init_weights_xavier_uniform_(self.decoder)\n",
    "        utils._submodules_init_weights_xavier_uniform_(self.fc_mu)\n",
    "        utils._xavier_uniform_(self.fc_var, -1.0)\n",
    "\n",
    "    #Call Encoder\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "\n",
    "    #Call Decoder\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    #Update parameters\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    #Forward pass\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trainer for data:\n",
    "\n",
    "    - Sets Hyperparameters\n",
    "    - Loads data\n",
    "    - trains data\n",
    "    \n",
    "    - Parameters:\n",
    "        - device, model, batch size, learning rate, epochs, starting kl value, \n",
    "            ending kl value, epoch to start annealing kl, number of annealing steps\n",
    "\n",
    "    - Constructor:\n",
    "        - Load model to device\n",
    "        - Set optimizer\n",
    "        - Initialize hyperparameters\n",
    "        - Setup tensorboard writer\n",
    "        - Load data \n",
    "\n",
    "    - Loss function:\n",
    "        - Calulcates MSE loss and KL divergence using mean reduction\n",
    "        - Returns these two values as tuple\n",
    "\n",
    "    - Training loop:\n",
    "        - Iterates over train_loader\n",
    "        - Calculates KL annealing rate\n",
    "        - Calls loss function \n",
    "        - Multiplies annealing rate by kl divergence\n",
    "        - Adds annealed kl loss and MSE loss for total loss\n",
    "        - Preforms norm clipping\n",
    "        - Writes to tensboard\n",
    "\n",
    "    - Annealing KL loss:\n",
    "        - Parameters:\n",
    "            - start_kl: initial weight of KL loss\n",
    "            - end_kl: end weight that KL loss builds up to\n",
    "            - annealing_start: epoch to start adding KL loss \n",
    "            - annealing_steps: number of epochs for KL loss to grow over\n",
    "        \n",
    "        - Training:\n",
    "            - Check for starting epoch \n",
    "            - If current epoch = annealing_start, anneal kl\n",
    "            - If current epoch != annealing_start, kl = 0\n",
    "    \n",
    "    - Usage:\n",
    "        - Create instance of trainer: trainer = VAETrainer(device)\n",
    "        - Call training loop function: trainer.train()\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import mmvae.trainers.utils as utils\n",
    "import torch.nn as nn\n",
    "import mmvae.models.Arch_Model as Arch_Model\n",
    "from mmvae.data import configure_singlechunk_dataloaders\n",
    "import torch.utils.tensorboard as tb\n",
    "\n",
    "class VAETrainer:\n",
    "    #Allow for possibility of sending specfifc hyperparameters into trainer\n",
    "    def __init__(self, device, model=Arch_Model.VAE(), batch_size=128, learning_rate=0.0001, num_epochs=10, start_kl=0.0, end_kl=1.0, annealing_start=0, annealing_steps=10):\n",
    "        #Configure\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        #Hyperparameters\n",
    "        self.lr = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.start_kl = start_kl #Initial Weight of KL loss\n",
    "        self.end_kl = end_kl    #End weight of KL loss\n",
    "        self.annealing_start = annealing_start  #Specify what epoch to start annealing kl \n",
    "        self.annealing_steps = annealing_steps  #Specify number of steps to anneal kl over\n",
    "        #Tensorboard writer \n",
    "        self.writer = tb.SummaryWriter()\n",
    "        #Load Data\n",
    "        self.train_loader = loaders.configure_singlechunk_dataloaders(\n",
    "            data_file_path='/active/debruinz_project/CellCensus_3M_Full/3m_human_full.npz',\n",
    "            metadata_file_path=None,\n",
    "            train_ratio=1,\n",
    "            batch_size=self.batch_size,\n",
    "            device=None\n",
    "        )\n",
    "\n",
    "    #Loss function returning MSE and KL divergence as a tuple using mean reduction\n",
    "    def loss_function(self, recon_x, x: torch.Tensor, mu, logvar):\n",
    "        reconstruction_loss = F.mse_loss(recon_x, x.to_dense(), reduction='mean') \n",
    "        kl_divergence = (-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())).mean()\n",
    "        return reconstruction_loss, kl_divergence\n",
    "\n",
    "    #Training loop\n",
    "    def train(self):\n",
    "        print(\"Start Training ....\")\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i, (x, _) in enumerate(self.train_loader):\n",
    "                x = x.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                recon_batch, mu, logvar = self.model(x)\n",
    "\n",
    "                #Check starting epoch for kl\n",
    "                annealing = 0\n",
    "                if epoch >= self.annealing_start:\n",
    "                    annealing_ratio = (epoch - self.annealing_start) / self.annealing_steps\n",
    "                    annealing = self.start_kl + annealing_ratio * (self.end_kl - self.start_kl)\n",
    "                \n",
    "                #Combine MSE and KL for total loss (kl * 0 if not at starting epoch)\n",
    "                recon_loss, kl_loss = self.loss_function(recon_batch, x, mu, logvar)\n",
    "                annealing_kl = kl_loss * annealing\n",
    "                loss = recon_loss + annealing_kl\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0) #Gradient Norm Clipping\n",
    "                self.optimizer.step()\n",
    "\n",
    "                #Tensorboard total loss over iterations\n",
    "                self.writer.add_scalar('Loss/Iteration', loss.item(), epoch * len(self.train_loader) + i) \n",
    "\n",
    "            #Write to tensorboard  \n",
    "            self.writer.add_scalar('Annealing Schedule', annealing, epoch) \n",
    "            self.writer.add_scalar('Loss/KL', kl_loss.item(), epoch)\n",
    "            self.writer.add_scalar('Loss/MSE', recon_loss.item(), epoch)\n",
    "            self.writer.add_scalar('Loss/Total', loss.item(), epoch)\n",
    "            \n",
    "        print(\"done training\")\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/active/debruinz_project/CellCensus_3M_Full/3m_human_full.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[1;32m---> 14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mVAETrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\hhami\\OneDrive\\Documents\\gvsu\\Machine Learning\\MMVAE_Architecture_Team\\notebooks\\..\\mmvae\\trainers\\Arch_Trainer.py:27\u001b[0m, in \u001b[0;36mVAETrainer.__init__\u001b[1;34m(self, device, model, batch_size, learning_rate, num_epochs, start_kl, end_kl, annealing_start, annealing_steps)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m tb\u001b[38;5;241m.\u001b[39mSummaryWriter()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#Load Data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m \u001b[43mconfigure_singlechunk_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/active/debruinz_project/CellCensus_3M_Full/3m_human_full.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hhami\\OneDrive\\Documents\\gvsu\\Machine Learning\\MMVAE_Architecture_Team\\notebooks\\..\\mmvae\\data\\loaders.py:150\u001b[0m, in \u001b[0;36mconfigure_singlechunk_dataloaders\u001b[1;34m(data_file_path, metadata_file_path, train_ratio, batch_size, test_batch_size, shuffle, device)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test_batch_size:\n\u001b[0;32m    148\u001b[0m     test_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m--> 150\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_data_and_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCellCensusDataSet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CellCensusDataset, collate_fn\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device:\n",
      "File \u001b[1;32mc:\\Users\\hhami\\OneDrive\\Documents\\gvsu\\Machine Learning\\MMVAE_Architecture_Team\\notebooks\\..\\mmvae\\data\\utils.py:17\u001b[0m, in \u001b[0;36msplit_data_and_metadata\u001b[1;34m(data_file_path, metadata_file_path, train_ratio, header)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_data_and_metadata\u001b[39m(data_file_path: \u001b[38;5;28mstr\u001b[39m, metadata_file_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, train_ratio: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Splits a csr_matrix and its corresponding metadata (pandas DataFrame) into training and validation sets based on a given ratio.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Important:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    :return: A tuple containing the training and validation sets for both the csr_matrix and the DataFrame.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     matrix: sp\u001b[38;5;241m.\u001b[39mcsr_matrix \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_npz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     split_data \u001b[38;5;241m=\u001b[39m train_ratio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m train_ratio \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata_file_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\sparse\\_matrix_io.py:134\u001b[0m, in \u001b[0;36mload_npz\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_npz\u001b[39m(file):\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Load a sparse array/matrix from a file using ``.npz`` format.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    >>> sparse_array = sp.sparse.csr_array(tmp)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mPICKLE_KWARGS\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m loaded:\n\u001b[0;32m    135\u001b[0m         sparse_format \u001b[38;5;241m=\u001b[39m loaded\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sparse_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/active/debruinz_project/CellCensus_3M_Full/3m_human_full.npz'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main file to call trainer:\n",
    "\n",
    "    - Sets device to cuda if available, if not use CPU\n",
    "    - Creater trainer and call train function\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from mmvae.trainers.Arch_Trainer import VAETrainer\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "    trainer = VAETrainer(device)\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
